import re
from datetime import datetime

from langchain_core.messages import HumanMessage, BaseMessage

from src.dev.memory.qa_agent_memory import MemoryManager
from src.dev.moddleware.qa_moddleware import DynamicModelManager
from src.dev.prompt.qa_prompt import QAPromptManager
from src.dev.retriever.konwage_retriever import KnowledgeRetriever
from src.dev.state.graph_state import GraphState
from src.dev.utils.scholar_tools import fetch_url_content, extract_file_content


def preprocess_node(state: GraphState) -> GraphState:
    """1. å‰ç½®å¤„ç†ï¼šæå–URLå’Œæ–‡ä»¶ä¿¡æ¯"""
    user_input = state["user_input"]
    print(f"ğŸš€ å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")

    # æå–URL
    url_pattern = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w\.-]*'
    urls = re.findall(url_pattern, user_input)

    if urls:
        print(f"ğŸ”— æ£€æµ‹åˆ°URL: {urls}")
        # æ¸…ç†è¾“å…¥ä¸­çš„URL
        for url in urls:
            user_input = user_input.replace(url, "").strip()
        state["url_content"] = "\n".join([fetch_url_content(url) for url in urls])

    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼ˆç®€åŒ–å¤„ç†ï¼Œå®é™…ä¸­éœ€è¦æ–‡ä»¶ä¸Šä¼ æœºåˆ¶ï¼‰
    file_pattern = r'(\.pdf|\.txt|\.docx?)$'
    if re.search(file_pattern, user_input, re.IGNORECASE):
        print("ğŸ“„ æ£€æµ‹åˆ°æ–‡ä»¶å¼•ç”¨")
        state["file_content"] = extract_file_content(user_input)

    state["processed_input"] = user_input
    return state

def type_classification_node(state: GraphState) -> GraphState:
    """1.4. ç±»å‹è¯†åˆ«ï¼šåˆ¤æ–­æ˜¯ä¸šåŠ¡é—®é¢˜è¿˜æ˜¯æ™®é€šé—®é¢˜"""
    print("ğŸ” è¿›è¡Œé—®é¢˜ç±»å‹è¯†åˆ«...")

    prompt_manager = QAPromptManager()
    model_manager = DynamicModelManager()

    prompt = prompt_manager.get_prompt(
        "type_classification",
        question=state["processed_input"]
    )

    model = model_manager.get_model()
    response = model.invoke(prompt)

    question_type = response.content.strip().lower()
    if "business" in question_type:
        state["question_type"] = "business"
    else:
        state["question_type"] = "general"

    print(f"ğŸ“Š è¯†åˆ«ç»“æœ: {state['question_type']}")
    return state

def summarize_input_node(state: GraphState) -> GraphState:
    """1.3. æ€»ç»“ä¿¡æ¯è·å–ç”¨æˆ·é—®é¢˜"""
    print("ğŸ“ æ€»ç»“ç”¨æˆ·é—®é¢˜...")

    # åˆå¹¶æ‰€æœ‰ä¿¡æ¯æº
    context_parts = []
    if state.get("url_content"):
        context_parts.append(f"URLå†…å®¹ï¼š{state['url_content']}")
    if state.get("file_content"):
        context_parts.append(f"æ–‡ä»¶å†…å®¹ï¼š{state['file_content']}")

    if context_parts:
        summary_context = "\n".join(context_parts)

        # ä½¿ç”¨æ¨¡å‹æ€»ç»“
        model = DynamicModelManager().get_model()
        summary_prompt = f"""
        è¯·æ€»ç»“ä»¥ä¸‹ä¿¡æ¯ï¼Œå¸®åŠ©ç†è§£ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼š

        ä¿¡æ¯ï¼š
        {summary_context}

        ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š
        {state['processed_input']}

        è¯·ç”¨ä¸€å¥è¯æ€»ç»“ç”¨æˆ·çš„æ ¸å¿ƒå…³åˆ‡ï¼š
        """

        response = model.invoke(summary_prompt)
        state["context"] = response.content
    else:
        state["context"] = state["processed_input"]

    print(f"âœ… æ€»ç»“å®Œæˆ: {state['context'][:100]}...")
    return state



def retrieve_context_node(state: GraphState) -> GraphState:
    """2.1.1/é€šç”¨æ£€ç´¢ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ä¸Šä¸‹æ–‡"""
    print("ğŸ” æ£€ç´¢ç›¸å…³çŸ¥è¯†...")

    retriever = KnowledgeRetriever()

    # æ„å»ºæŸ¥è¯¢
    query = state["context"]
    if state.get("question_type") == "business":
        query = f"é‡‘èä¸šåŠ¡: {query}"

    # æ£€ç´¢
    retrieved = retriever.retrieve(query)

    if retrieved:
        state["retrieval_result"] = retrieved
        print(f"âœ… æ£€ç´¢åˆ° {len(retrieved.split())} ä¸ªè¯çš„ä¸Šä¸‹æ–‡")
    else:
        state["retrieval_result"] = ""
        print("âš ï¸  æœªæ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡")

    return state


# ============== 8. ä¸šåŠ¡å›ç­”èŠ‚ç‚¹ ==============
def answer_business_question_node(state: GraphState) -> GraphState:
    """2.1. å›ç­”å®¢æˆ·ä¸šåŠ¡ä¿¡æ¯"""
    print("ğŸ¦ ç”Ÿæˆä¸šåŠ¡é—®é¢˜å›ç­”...")

    prompt_manager = QAPromptManager()
    model_manager = DynamicModelManager()

    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context = ""
    if state.get("retrieval_result"):
        context += f"çŸ¥è¯†åº“ä¿¡æ¯ï¼š\n{state['retrieval_result']}\n\n"
    if state.get("context"):
        context += f"é—®é¢˜æ€»ç»“ï¼š\n{state['context']}"

    # è·å–åŠ¨æ€æç¤ºè¯
    prompt = prompt_manager.get_prompt(
        "business",
        context=context,
        question=state["processed_input"]
    )

    # é€‰æ‹©æ¨¡å‹
    model = model_manager.select_model_based_on_type("business")

    # ç”Ÿæˆå›ç­”
    response = model.invoke(prompt)
    state["answer"] = response.content

    print(f"âœ… ä¸šåŠ¡å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(state['answer'])} å­—ç¬¦")
    return state



# ============== 9. æ™®é€šå›ç­”èŠ‚ç‚¹ ==============
def answer_general_question_node(state: GraphState) -> GraphState:
    """2.2. å›ç­”å®¢æˆ·æ™®é€šé—®é¢˜"""
    print("ğŸ’¬ ç”Ÿæˆæ™®é€šé—®é¢˜å›ç­”...")

    prompt_manager = QAPromptManager()
    model_manager = DynamicModelManager()

    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context = ""
    if state.get("retrieval_result"):
        context += f"ç›¸å…³çŸ¥è¯†ï¼š\n{state['retrieval_result']}\n\n"
    if state.get("context"):
        context += f"é—®é¢˜èƒŒæ™¯ï¼š\n{state['context']}"

    # è·å–åŠ¨æ€æç¤ºè¯
    prompt = prompt_manager.get_prompt(
        "general",
        context=context,
        question=state["processed_input"]
    )

    # é€‰æ‹©æ¨¡å‹
    model = model_manager.select_model_based_on_type("general")

    # ç”Ÿæˆå›ç­”
    response = model.invoke(prompt)
    state["answer"] = response.content

    print(f"âœ… æ™®é€šå›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(state['answer'])} å­—ç¬¦")
    return state


# ============== 10. ç­”æ¡ˆæ ¡éªŒèŠ‚ç‚¹ ==============
def validate_answer_node(state: GraphState) -> GraphState:
    """2.3. æ ¡éªŒç­”æ¡ˆ"""
    print("âœ… æ ¡éªŒç­”æ¡ˆè´¨é‡...")

    prompt_manager = QAPromptManager()
    model_manager = DynamicModelManager()

    prompt = prompt_manager.get_prompt(
        "validation",
        question=state["processed_input"],
        answer=state["answer"]
    )

    model = model_manager.get_model()
    response = model.invoke(prompt)

    validation_result = response.content.strip()

    if "é€šè¿‡" in validation_result:
        state["answer_validated"] = True
        print("ğŸ‰ ç­”æ¡ˆéªŒè¯é€šè¿‡")
    else:
        state["answer_validated"] = False
        print("âš ï¸  ç­”æ¡ˆéªŒè¯ä¸é€šè¿‡ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")

    return state


# ============== 11. åç½®å¤„ç†èŠ‚ç‚¹ ==============
def postprocess_output_node(state: GraphState) -> GraphState:
    """3. END: åç½®å¤„ç†"""
    print("ğŸ”§ è¿›è¡Œåç½®å¤„ç†...")

    # æ·»åŠ å›ç­”åˆ°æ¶ˆæ¯å†å²
    state["messages"].append(HumanMessage(content=state["user_input"]))
    state["messages"].append(
        BaseMessage(content=state["answer"], type="assistant")
    )

    # ä¿å­˜åˆ°è®°å¿†
    memory_manager = MemoryManager()
    memory_manager.save_memory(
        state["session_id"],
        {
            "timestamp": datetime.now().isoformat(),
            "user_input": state["user_input"],
            "answer": state["answer"],
            "question_type": state["question_type"],
            "validated": state["answer_validated"]
        }
    )

    # æ¸…ç†ä¸´æ—¶çŠ¶æ€
    state["user_input"] = ""

    print("âœ… åç½®å¤„ç†å®Œæˆ")
    return state