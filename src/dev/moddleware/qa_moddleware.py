import os
from pathlib import Path
from typing import Optional, Dict, Any, Type
from dotenv import load_dotenv
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_deepseek import ChatDeepSeek
from langchain_qwq import ChatQwen

class DynamicModelManager:
    """
    LangChain æ¨¡å‹å®ä¾‹ç®¡ç†å™¨ï¼ˆæ ¸å¿ƒï¼šåªäº§å‡º Model å®ä¾‹ï¼Œä¸æ‰§è¡Œæ¨ç†ï¼‰
    ä½œç”¨ï¼šæ ¹æ®æ¨¡å‹åˆ«åï¼Œä».envåŠ è½½é…ç½®ï¼Œè¿”å›å¯¹åº”çš„ LangChain ChatModel å®ä¾‹
    """
    # æ¨¡å‹æ˜ å°„ï¼šåˆ«å â†’ (LangChainæ¨¡å‹ç±», é…ç½®å‰ç¼€, å‚å•†åŸç”Ÿæ¨¡å‹å)
    MODEL_MAPPING: Dict[str, tuple[Type[BaseChatModel], str, str]] = {
        # OpenAIç³»åˆ—
        "gpt-3.5-turbo": (ChatOpenAI, "OPENAI", "gpt-3.5-turbo"),
        "gpt-4o": (ChatOpenAI, "OPENAI", "gpt-4o"),
        # Anthropic Claude
        "claude-3-haiku": (ChatAnthropic, "ANTHROPIC", "claude-3-haiku-20240307"),
        # é˜¿é‡Œé€šä¹‰åƒé—®ï¼ˆå…¼å®¹OpenAIæ¥å£ï¼‰
        "qwen-turbo": (ChatQwen, "ChatQwen", "qwen-turbo"),
        # deepseek
        "deepseek": (ChatDeepSeek, "DEEPSEEK", "deepseek-chat"),
        # é»˜è®¤
        "default": (ChatOpenAI, "OPENAI", "gpt-3.5-turbo"),
    }

    def __init__(self, override_config: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨ï¼ˆç¡®ä¿åŠ è½½åˆ°é¡¹ç›®æ ¹ç›®å½•çš„.envæ–‡ä»¶ï¼‰
        :param override_config: å…¨å±€é…ç½®è¦†ç›–ï¼ˆå¦‚{"OPENAI_BASE_URL": "ä»£ç†åœ°å€"}ï¼‰
        """
        # ğŸŒŸ æ ¸å¿ƒä¿®å¤ï¼šè·å–é¡¹ç›®æ ¹ç›®å½•çš„.envç»å¯¹è·¯å¾„ï¼ˆå…³é”®ï¼ï¼‰
        # æ­¥éª¤1ï¼šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆæ ¹æ®å®é™…ç›®å½•ç»“æ„è°ƒæ•´ï¼Œæ¯”å¦‚å‘ä¸Šæ‰¾åŒ…å«.envçš„ç›®å½•ï¼‰
        # æ–¹æ³•ï¼šä»å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œå‘ä¸Šå›æº¯åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾.envåœ¨é¡¹ç›®æ ¹ï¼‰
        current_file = Path(__file__).resolve()  # å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
        project_root = current_file.parents[3]  # æŒ‰éœ€è°ƒæ•´å±‚çº§ï¼š0=å½“å‰æ–‡ä»¶ç›®å½•ï¼Œ1=ä¸Šä¸€çº§ï¼Œä¾æ­¤ç±»æ¨
        env_path = project_root / ".env"

        # æ­¥éª¤2ï¼šåŠ è½½.envå¹¶éªŒè¯æ˜¯å¦æˆåŠŸ
        load_success = load_dotenv(dotenv_path=env_path, override=True)  # override=Trueï¼šè¦†ç›–ç³»ç»Ÿç¯å¢ƒå˜é‡
        if not load_success:
            # å¯é€‰ï¼šè­¦å‘Šä½†ä¸æŠ¥é”™ï¼ˆæˆ–æŠ›å¼‚å¸¸ï¼Œæ ¹æ®éœ€æ±‚ï¼‰
            print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°.envæ–‡ä»¶ï¼ˆè·¯å¾„ï¼š{env_path}ï¼‰ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")
        else:
            print(f"æˆåŠŸåŠ è½½.envæ–‡ä»¶ï¼š{env_path}")

        # æ­¥éª¤3ï¼šåˆå§‹åŒ–è¦†ç›–é…ç½®
        self.override_config = override_config or {}

    def get_model(
        self,
        model_name: Optional[str] = None,
        model_kwargs: Optional[Dict[str, Any]] = None
    ) -> BaseChatModel:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šè·å–æŒ‡å®šæ¨¡å‹çš„ LangChain ChatModel å®ä¾‹
        :param model_name: æ¨¡å‹åˆ«åï¼ˆå¦‚gpt-3.5-turboã€ernie-3.5ï¼‰
        :param model_kwargs: æ¨¡å‹åˆå§‹åŒ–å‚æ•°ï¼ˆå¦‚temperatureã€max_tokensï¼‰
        :return: LangChain BaseChatModel å®ä¾‹
        """
        # 1. æ ¡éªŒæ¨¡å‹æ˜¯å¦æ”¯æŒ
        if model_name not in self.MODEL_MAPPING:
            model_name = "gpt-3.5-turbo"
            # raise ValueError(
            #     f"ä¸æ”¯æŒçš„æ¨¡å‹ï¼š{model_name}ï¼Œæ”¯æŒåˆ—è¡¨ï¼š{list(self.MODEL_MAPPING.keys())}"
            # )

        # 2. è·å–æ¨¡å‹é…ç½®
        chat_model_cls, config_prefix, real_model_name = self.MODEL_MAPPING[model_name]
        model_kwargs = model_kwargs or {"temperature": 0.1}

        # 3. åŠ è½½é…ç½®ï¼ˆ.env + è¦†ç›–é…ç½®ï¼‰
        api_key = self._get_config(f"{config_prefix}_API_KEY")
        base_url = self._get_config(f"{config_prefix}_BASE_URL", required=False)

        # 4. æ„å»ºæ¨¡å‹åˆå§‹åŒ–å‚æ•°
        init_kwargs = {
            "model_name": real_model_name,
            "api_key": api_key,
            **model_kwargs
        }
        if base_url:
            init_kwargs["base_url"] = base_url

        # 5. è¿”å› LangChain Model å®ä¾‹ï¼ˆæ ¸å¿ƒï¼åªè¿”å›å®ä¾‹ï¼Œä¸æ‰§è¡Œæ¨ç†ï¼‰
        return chat_model_cls(**init_kwargs)

    def _get_config(self, key: str, required: bool = True) -> Optional[str]:
        """è·å–é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šoverride_config > .env > Noneï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨è¦†ç›–é…ç½®
        if key in self.override_config:
            return self.override_config[key]
        # å…¶æ¬¡ä».envåŠ è½½
        env_value = os.getenv(key)
        # æ ¡éªŒå¿…å¡«é…ç½®
        if required and not env_value:
            raise RuntimeError(f"è¯·åœ¨.envä¸­é…ç½® {key} ç¯å¢ƒå˜é‡ï¼")
        return env_value

    @classmethod
    def register_model(
        cls,
        alias: str,
        chat_model_cls: Type[BaseChatModel],
        config_prefix: str,
        real_model_name: str
    ):
        """åŠ¨æ€æ³¨å†Œæ–°æ¨¡å‹ï¼ˆæ‰©å±•ç”¨ï¼‰"""
        cls.MODEL_MAPPING[alias] = (chat_model_cls, config_prefix, real_model_name)