import os
import logging
import uuid
from datetime import datetime
from pathlib import Path
import requests
from typing import List, Optional, Iterable, Tuple
from langchain_community.document_loaders import PyPDFLoader

import chardet
import yaml
from langchain_community.document_loaders import WebBaseLoader, PyMuPDFLoader, TextLoader, UnstructuredMarkdownLoader, \
    Docx2txtLoader, UnstructuredImageLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm import tqdm  # è¿›åº¦æ¡ï¼Œæå‡ä½“éªŒ

# å¯¼å…¥ä½ å·²æœ‰çš„åŠ è½½å‡½æ•°å’Œä¾èµ–
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings  # è‹¥ç”¨å¼€æºæ¨¡å‹ï¼Œæ›¿æ¢ä¸ºHuggingFaceEmbeddings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("document_vector_store")

from config import config


# åŠ è½½é…ç½®æ–‡ä»¶
def load_config(config_path: str = "config.yaml") -> dict:
    """åŠ è½½config.yamlé…ç½®"""
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶ï¼š{config_path}")
        return config
    except FileNotFoundError:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}")
        raise
    except Exception as e:
        logger.error(f"âŒ åŠ è½½é…ç½®å¤±è´¥ï¼š{str(e)}")
        raise


# é‡‘èæ–‡æœ¬ä¼˜åŒ–çš„åˆ‡åˆ†å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=config["CHUNK_SIZE"],  # å­—å…¸é”®å–å€¼
    chunk_overlap=config["CHUNK_OVERLAP"],
    separators=config["SEPARATORS"],
    length_function=len
)

# é‡‘èé¢†åŸŸåµŒå…¥æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name=config["EMBEDDING_MODEL"],
    model_kwargs={"device": "cuda" if config["EMBEDDING_USE_GPU"] else "cpu"},  # åŒºåˆ†åµŒå…¥æ¨¡å‹çš„GPUé…ç½®
    encode_kwargs={"normalize_embeddings": True}
)


def _detect_encoding(file_path: str) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç """
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # è¯»å–éƒ¨åˆ†æ•°æ®ç”¨äºæ£€æµ‹
        detected = chardet.detect(raw_data)
        return detected['encoding'] or 'utf-8'
    except Exception:
        return 'utf-8'  # æ£€æµ‹å¤±è´¥é»˜è®¤ä½¿ç”¨utf-8


def _get_loader_and_metadata(file_path: str, encoding: str) -> Tuple[object, dict]:
    """è·å–æ–‡ä»¶åŠ è½½å™¨å’Œå¯¹åº”çš„å…ƒæ•°æ®"""
    base_metadata = {}

    if file_path.startswith(("http://", "https://")):
        # å¤„ç†URL
        loader = WebBaseLoader(
            file_path,
            verify_ssl=False,
            encoding=encoding
        )
        base_metadata["file_type"] = "html"
        return loader, base_metadata

    # å¤„ç†æœ¬åœ°æ–‡ä»¶
    file_path_obj = Path(file_path)
    suffix = file_path_obj.suffix.lower()

    # è¡¥å……æ–‡ä»¶åŸºæœ¬å…ƒæ•°æ®
    try:
        stat = file_path_obj.stat()
        base_metadata["file_size"] = f"{stat.st_size} bytes"
        base_metadata["file_modified"] = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        base_metadata["file_size"] = "unknown"
        base_metadata["file_modified"] = "unknown"

    # æ ¹æ®åç¼€é€‰æ‹©åŠ è½½å™¨
    if suffix == ".pdf":
        loader = PyPDFLoader(file_path)
        base_metadata["file_type"] = "pdf"
    elif suffix == ".txt":
        # è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        used_encoding = encoding if encoding else _detect_encoding(file_path)
        loader = TextLoader(file_path, encoding=used_encoding)
        base_metadata["file_type"] = "txt"
        base_metadata["encoding"] = used_encoding
    elif suffix == ".md":
        loader = UnstructuredMarkdownLoader(file_path, encoding=encoding)
        base_metadata["file_type"] = "md"
    elif suffix == ".docx":
        loader = Docx2txtLoader(file_path)
        base_metadata["file_type"] = "docx"
    elif suffix in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
        loader = UnstructuredImageLoader(
            file_path
        )
        base_metadata["file_type"] = "image"
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{suffix}")

    return loader, base_metadata


def load_document(
        file_path: str,
        source_name: Optional[str] = None,
        encoding: str = ""  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
) -> List[Document]:
    """
    åŸºäºLangChainå†…ç½®Loaderçš„é€šç”¨æ–‡ä»¶åŠ è½½æ–¹æ³•
    è‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼šPDF/HTML/URL/TXT/MD/DOCX/å›¾ç‰‡ï¼ˆJPG/PNGç­‰ï¼‰
    :param file_path: æ–‡ä»¶è·¯å¾„/URLï¼ˆURLéœ€ä»¥http/httpså¼€å¤´ï¼‰
    :param source_name: è‡ªå®šä¹‰æ¥æºåç§°ï¼ˆç”¨äºæ ‡æ³¨å¼•ç”¨ï¼‰
    :param encoding: æ–‡æœ¬ç¼–ç ï¼ˆç©ºå­—ç¬¦ä¸²åˆ™è‡ªåŠ¨æ£€æµ‹ï¼Œä¸»è¦ç”¨äºTXTæ–‡ä»¶ï¼‰
    :return: å¸¦é‡‘èå…ƒæ•°æ®çš„Documentåˆ—è¡¨ï¼ˆå·²åˆ‡åˆ†ï¼‰
    """
    # åŸºç¡€å…ƒæ•°æ®ï¼ˆæ‰€æœ‰æ ¼å¼é€šç”¨ï¼‰
    base_metadata = {
        "source": source_name or file_path,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "doc_id": str(uuid.uuid4()),
    }

    try:
        # è·å–åŠ è½½å™¨å’Œç±»å‹å…ƒæ•°æ®
        loader, type_metadata = _get_loader_and_metadata(file_path, encoding)
        # åˆå¹¶å…ƒæ•°æ®
        base_metadata.update(type_metadata)

        # åŠ è½½æ–‡æ¡£ï¼ˆé’ˆå¯¹éœ€è¦èµ„æºç®¡ç†çš„åŠ è½½å™¨ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
        if isinstance(loader, PyMuPDFLoader):
            with loader:
                raw_docs = loader.load()
        else:
            raw_docs = loader.load()

        if not raw_docs:
            return []

        # è¡¥å……å…ƒæ•°æ®å¹¶åˆ‡åˆ†
        enhanced_docs = []
        for i, doc in enumerate(raw_docs):
            # åˆå¹¶å…ƒæ•°æ®æ—¶ï¼Œä¿ç•™åŸæ–‡æ¡£çš„é¡µç ï¼ˆå¦‚PDFçš„pageå­—æ®µï¼‰
            merged_metadata = {**doc.metadata, **base_metadata, "chunk_raw_id": i}
            # ç¡®ä¿é¡µç å­—æ®µå­˜åœ¨ï¼ˆé’ˆå¯¹PDFç­‰æ ¼å¼ï¼‰
            if "page" not in merged_metadata:
                merged_metadata["page"] = i + 1  # é»˜è®¤ä¸ºæ–‡æ¡£ä¸­çš„ç¬¬i+1é¡µ
            enhanced_docs.append(Document(page_content=doc.page_content, metadata=merged_metadata))

        split_docs = text_splitter.split_documents(enhanced_docs)

        # è¡¥å……chunkå”¯ä¸€æ ‡è¯†
        for i, doc in enumerate(split_docs):
            doc.metadata["chunk_id"] = f"{doc.metadata['doc_id']}_{i}"
            doc.metadata["chunk_total"] = len(split_docs)  # æ€»chunkæ•°

        return split_docs

    except FileNotFoundError:
        raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    except PermissionError:
        raise ValueError(f"æ— æƒé™è®¿é—®æ–‡ä»¶ï¼š{file_path}")
    except requests.exceptions.RequestException as e:
        raise ValueError(f"URLè¯·æ±‚å¤±è´¥ï¼ˆ{file_path}ï¼‰ï¼š{str(e)}")
    except Exception as e:
        raise ValueError(f"æ–‡ä»¶åŠ è½½å¤±è´¥ï¼ˆ{file_path}ï¼‰ï¼š{str(e)}")


def format_document(sources: List[Document]) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨æ¥æºï¼ˆé€‚é…LangChain Loaderçš„å…ƒæ•°æ®ï¼‰"""
    if not sources:
        return "æ— å¼•ç”¨æ¥æº"

    source_info = []
    for i, doc in enumerate(sources, 1):
        meta = doc.metadata
        source_type = meta.get("file_type", "unknown")
        source = meta.get("source", "unknown")
        timestamp = meta.get("timestamp", "unknown")

        format_map = {
            "pdf": f"{i}. PDFæ–‡æ¡£ï¼š{source}ï¼ˆé¡µç ï¼š{meta.get('page', 'æœªçŸ¥')}ï¼‰- æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "html": f"{i}. ç½‘é¡µèµ„è®¯ï¼š{source}ï¼ˆæ ‡é¢˜ï¼š{meta.get('title', 'æœªçŸ¥')}ï¼‰- æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "txt": f"{i}. çº¯æ–‡æœ¬æ–‡æ¡£ï¼š{source}ï¼ˆç¼–ç ï¼š{meta.get('encoding', 'utf-8')}ï¼‰- æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "md": f"{i}. Markdownæ–‡æ¡£ï¼š{source} - æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "docx": f"{i}. Wordæ–‡æ¡£ï¼š{source} - æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "image": f"{i}. å›¾ç‰‡æ–‡ä»¶ï¼š{source}ï¼ˆOCRæå–ï¼‰- æŠ“å–æ—¶é—´ï¼š{timestamp}",
            "unknown": f"{i}. æœªçŸ¥æ¥æºï¼š{source} - æŠ“å–æ—¶é—´ï¼š{timestamp}"
        }
        source_info.append(format_map.get(source_type, format_map["unknown"]))

    return "\n".join(source_info)


class DocumentVectorStore:
    """æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨ç®¡ç†å™¨ï¼šåˆ†ç¦»ã€ŒåŠ è½½ã€å’Œã€Œå­˜å‚¨ã€é€»è¾‘"""

    def __init__(self, config: dict, vector_store_path: str = "./chroma_db"):
        self.config = config
        self.vector_store_path = vector_store_path
        self.supported_extensions = set(config["LOADER"]["SUPPORTED_EXTENSIONS"])
        self.embeddings = self._init_embeddings()
        self.vector_store = self._init_vector_store()

    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå…¼å®¹OpenAI/å¼€æºæ¨¡å‹ï¼‰"""
        # æ–¹å¼1ï¼šä½¿ç”¨OpenAI Embeddingsï¼ˆéœ€é…ç½®OPENAI_API_KEYï¼‰
        try:
            return OpenAIEmbeddings()
        except Exception:
            # æ–¹å¼2ï¼šä½¿ç”¨å¼€æºä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ˆä»é…ç½®è¯»å–ï¼‰
            from langchain_community.embeddings import HuggingFaceEmbeddings
            return HuggingFaceEmbeddings(
                model_name=self.config["EMBEDDING_MODEL"],
                model_kwargs={"device": "cuda" if self.config["OCR_USE_GPU"] else "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )

    def _init_vector_store(self):
        """åˆå§‹åŒ–Chromaå‘é‡å­˜å‚¨ï¼ˆé¿å…ç±»åå†²çª+æ­£ç¡®å®ä¾‹åŒ–ï¼‰"""
        try:
            # å˜é‡åç”¨vector_storeï¼Œé¿å…å’ŒChromaç±»åå†²çª
            vector_store = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
            logger.info(f"âœ… æˆåŠŸåŠ è½½å‘é‡åº“ï¼š{self.vector_store_path}")
            return vector_store
        except Exception as e:
            logger.warning(f"âš ï¸  åŠ è½½å‘é‡åº“å¤±è´¥ï¼Œåˆ›å»ºæ–°åº“ï¼š{str(e)}")
            # from_textsæ˜¯ç±»æ–¹æ³•ï¼Œè¿”å›å®ä¾‹ï¼Œæ— è°ƒç”¨å†²çª
            return Chroma.from_texts(
                texts=["å‘é‡åº“åˆå§‹åŒ–å ä½ç¬¦"],
                embedding=self.embeddings,
                persist_directory=self.vector_store_path
            )

    def scan_directory(self, dir_path: str, recursive: bool = True) -> List[str]:
        """
        æ‰«ææŒ‡å®šç›®å½•ï¼Œè¿”å›æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶è·¯å¾„
        :param dir_path: ç›®æ ‡ç›®å½•
        :param recursive: æ˜¯å¦é€’å½’éå†å­ç›®å½•
        :return: ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not os.path.exists(dir_path):
            logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨ï¼š{dir_path}")
            return []

        file_paths = []
        walk_iter = os.walk(dir_path) if recursive else [(dir_path, [], os.listdir(dir_path))]

        for root, _, files in walk_iter:
            for file in files:
                file_path = Path(root) / file
                # è¿‡æ»¤æ”¯æŒçš„æ–‡ä»¶åç¼€
                if file_path.suffix.lower() in self.supported_extensions:
                    # è¿‡æ»¤è¶…å¤§æ–‡ä»¶ï¼ˆä»é…ç½®è¯»å–é˜ˆå€¼ï¼‰
                    max_size = self.config["LOADER"]["MAX_FILE_SIZE_MB"] * 1024 * 1024
                    if file_path.stat().st_size <= max_size:
                        file_paths.append(str(file_path))
                    else:
                        logger.warning(
                            f"âš ï¸ æ–‡ä»¶è¿‡å¤§è·³è¿‡ï¼š{file_path}ï¼ˆå¤§å°ï¼š{file_path.stat().st_size / 1024 / 1024:.2f}MBï¼‰")

        logger.info(f"âœ… æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(file_paths)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")
        return file_paths

    def batch_load_documents(self, file_paths: List[str]) -> List[Document]:
        """
        æ‰¹é‡åŠ è½½æ–‡ä»¶ï¼ˆè°ƒç”¨ä½ çš„load_documentï¼‰ï¼Œå•ä¸ªæ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“
        :param file_paths: æ‰«æå¾—åˆ°çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        :return: æ‰€æœ‰åŠ è½½æˆåŠŸçš„Documentåˆ—è¡¨
        """
        all_docs = []
        failed_files = []

        for file_path in tqdm(file_paths, desc="ğŸ“„ æ‰¹é‡åŠ è½½æ–‡ä»¶"):
            try:
                # è°ƒç”¨ä½ å·²æœ‰çš„load_documentæ–¹æ³•
                docs = load_document(
                    file_path=file_path,
                    source_name=file_path,
                    encoding=""  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                )
                if docs:
                    all_docs.extend(docs)
                    logger.info(f"âœ… åŠ è½½æˆåŠŸï¼š{file_path}ï¼ˆç”Ÿæˆ {len(docs)} ä¸ªæ–‡æœ¬å—ï¼‰")
                else:
                    logger.warning(f"âš ï¸ æ— å†…å®¹ï¼š{file_path}")
            except Exception as e:
                failed_files.append((file_path, str(e)))
                logger.error(f"âŒ åŠ è½½å¤±è´¥ï¼š{file_path} - {str(e)}")

        # è¾“å‡ºåŠ è½½ç»Ÿè®¡
        logger.info(f"\nğŸ“Š æ‰¹é‡åŠ è½½ç»Ÿè®¡ï¼š")
        logger.info(f"   æ€»æ–‡ä»¶æ•°ï¼š{len(file_paths)}")
        logger.info(f"   æˆåŠŸåŠ è½½ï¼š{len(file_paths) - len(failed_files)}")
        logger.info(f"   å¤±è´¥æ•°ï¼š{len(failed_files)}")
        logger.info(f"   ç”Ÿæˆæ–‡æœ¬å—æ€»æ•°ï¼š{len(all_docs)}")

        if failed_files:
            logger.warning(f"âŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨ï¼š{[f[0] for f in failed_files]}")

        return all_docs

    def store_embeddings(self, docs: Iterable[Document], batch_size: int = 100):
        """
        å‘é‡åŒ–å­˜å‚¨ï¼ˆæ”¯æŒå•ç‹¬æ‰§è¡Œï¼‰
        :param docs: åŠ è½½å®Œæˆçš„Documentåˆ—è¡¨ï¼ˆå¯æ¥è‡ªä»»æ„æ¥æºï¼Œä¸ä¸€å®šæ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
        :param batch_size: æ‰¹é‡å­˜å‚¨å¤§å°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        """
        if not docs:
            logger.warning("âš ï¸ æ— æ–‡æ¡£å¯å­˜å‚¨")
            return

        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå…¼å®¹è¿­ä»£å™¨ï¼‰
        docs_list = list(docs)
        logger.info(f"ğŸ“¥ å¼€å§‹å‘é‡åŒ–å­˜å‚¨ï¼Œå…± {len(docs_list)} ä¸ªæ–‡æœ¬å—ï¼Œæ‰¹é‡å¤§å°ï¼š{batch_size}")

        # æ‰¹é‡å­˜å‚¨
        for i in tqdm(range(0, len(docs_list), batch_size), desc="ğŸ” å‘é‡åŒ–å­˜å‚¨"):
            batch_docs = docs_list[i:i + batch_size]
            ids = [f"doc_{uuid.uuid4()}" for _ in batch_docs]

            # ç›´æ¥ä¼  Document åˆ—è¡¨ï¼Œæ— éœ€æ‰‹åŠ¨æ‹†åˆ†
            self.vector_store.add_documents(
                documents=batch_docs,
                ids=ids
            )

        logger.info(f"âœ… å‘é‡åŒ–å­˜å‚¨å®Œæˆï¼Œå…±å­˜å‚¨ {len(docs_list)} ä¸ªæ–‡æœ¬å—")

    def run_full_pipeline(self, dir_path: str, recursive: bool = True):
        """
        ä¸€é”®æ‰§è¡Œï¼šæ‰«æç›®å½• â†’ æ‰¹é‡åŠ è½½ â†’ å‘é‡åŒ–å­˜å‚¨ï¼ˆæ•´åˆæµç¨‹ï¼‰
        """
        logger.info("\nğŸš€ å¼€å§‹æ‰§è¡Œã€Œæ‰«æâ†’åŠ è½½â†’å­˜å‚¨ã€å…¨æµç¨‹")
        # 1. æ‰«æç›®å½•
        file_paths = self.scan_directory(dir_path, recursive)
        if not file_paths:
            logger.error("âŒ æ— æ”¯æŒçš„æ–‡ä»¶ï¼Œæµç¨‹ç»ˆæ­¢")
            return

        # 2. æ‰¹é‡åŠ è½½
        docs = self.batch_load_documents(file_paths)
        if not docs:
            logger.error("âŒ æ— åŠ è½½æˆåŠŸçš„æ–‡æ¡£ï¼Œæµç¨‹ç»ˆæ­¢")
            return

        # 3. å‘é‡åŒ–å­˜å‚¨
        self.store_embeddings(docs)
        logger.info("\nğŸ‰ å…¨æµç¨‹æ‰§è¡Œå®Œæˆï¼")


# ------------------- å•ç‹¬æ‰§è¡Œç¤ºä¾‹ -------------------
if __name__ == "__main__":
    # 1. åŠ è½½é…ç½®

    # 2. åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
    vector_store = DocumentVectorStore(config, vector_store_path="../../../vector_data/chroma")

    # æ–¹å¼1ï¼šæ‰§è¡Œå…¨æµç¨‹ï¼ˆæ‰«æ+åŠ è½½+å­˜å‚¨ï¼‰
    vector_store.run_full_pipeline(
        dir_path="../../../fintech_file",  # ä½ çš„æ–‡æ¡£ç›®å½•
        recursive=True  # é€’å½’éå†å­ç›®å½•
    )

    # æ–¹å¼2ï¼šå•ç‹¬æ‰§è¡Œå‘é‡åŒ–å­˜å‚¨ï¼ˆæ¯”å¦‚åŠ è½½å¥½çš„docsåˆ—è¡¨ï¼‰
    # docs = [Document(page_content="æµ‹è¯•æ–‡æœ¬", metadata={"source": "test.txt"})]
    # vector_store.store_embeddings(docs)

    # æ–¹å¼3ï¼šåˆ†æ­¥æ‰§è¡Œ
    # file_paths = vector_store.scan_directory("./docs")
    # docs = vector_store.batch_load_documents(file_paths)
    # vector_store.store_embeddings(docs)
